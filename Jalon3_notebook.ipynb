{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "865a9207",
   "metadata": {},
   "source": [
    "# JALON 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af13ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import contractions\n",
    "\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "# PREPROCESS\n",
    "topics = {0:'service',1:'price',2:'pizza',3:'order',4:'food',5:'wait',6:'place',7:'burger',8:'taste',9:'chicken',10:'bar',11:'time',12:'go',13:'lunch',14:'restaurant'}\n",
    "negative_words = ['not', 'no', 'never', 'nor', 'hardly', 'barely']\n",
    "negative_prefix = \"NOT_\"\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def tokenize_text(text):\n",
    "    text_processed = \" \".join(tokenizer.tokenize(text))\n",
    "    return text_processed\n",
    "\n",
    "\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load(disable=['parser', 'tagger', 'ner'])\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \n",
    "    tokens_tagged = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "    lemmatized_text_list = list()\n",
    "    \n",
    "    for word, tag in tokens_tagged:\n",
    "        if tag.startswith('J'):\n",
    "            lemmatized_text_list.append(lemmatizer.lemmatize(word,'a')) # Lemmatise adjectives. Not doing anything since we remove all adjective\n",
    "        elif tag.startswith('V'):\n",
    "            lemmatized_text_list.append(lemmatizer.lemmatize(word,'v')) # Lemmatise verbs\n",
    "        elif tag.startswith('N'):\n",
    "            lemmatized_text_list.append(lemmatizer.lemmatize(word,'n')) # Lemmatise nouns\n",
    "        elif tag.startswith('R'):\n",
    "            lemmatized_text_list.append(lemmatizer.lemmatize(word,'r')) # Lemmatise adverbs\n",
    "        else:\n",
    "            lemmatized_text_list.append(lemmatizer.lemmatize(word)) # If no tags has been found, perform a non specific lemmatisation\n",
    "    \n",
    "    return \" \".join(lemmatized_text_list)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "def normalize_text(text):\n",
    "    return \" \".join([word.lower() for word in text.split()])\n",
    "\n",
    "\n",
    "def contraction_text(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "\n",
    "def get_negative_token(text):\n",
    "    tokens = text.split()\n",
    "    negative_idx = [i+1 for i in range(len(tokens)-1) if tokens[i] in negative_words]\n",
    "    for idx in negative_idx:\n",
    "        if idx < len(tokens):\n",
    "            tokens[idx]= negative_prefix + tokens[idx]\n",
    "    \n",
    "    tokens = [token for i,token in enumerate(tokens) if i+1 not in negative_idx]\n",
    "    \n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    english_stopwords = stopwords.words(\"english\") + list(STOP_WORDS) + [\"tell\", \"restaurant\"]\n",
    "    \n",
    "    return \" \".join([word for word in text.split() if word not in english_stopwords])\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    # Tokenize review\n",
    "    text = tokenize_text(text)\n",
    "    \n",
    "    # Lemmatize review\n",
    "    text = lemmatize_text(text)\n",
    "    \n",
    "    # Normalize review\n",
    "    text = normalize_text(text)\n",
    "    \n",
    "    # Remove contractions\n",
    "    text = contraction_text(text)\n",
    "\n",
    "    # Get negative tokens\n",
    "    text = get_negative_token(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    text = remove_stopwords(text)\n",
    "    \n",
    "    return text\n",
    "    \n",
    "\n",
    "\n",
    "# In[106]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "#PREDICT\n",
    "import pickle \n",
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "with open(r\"pickle_files_nmfv1\",\"rb\") as input_file:\n",
    "    model = pickle.load(input_file)\n",
    "    \n",
    "with open(r\"pickle_files_vectoriseurv1\",\"rb\") as input_file:\n",
    "    vectoriseur = pickle.load(input_file)\n",
    "list_topics_final = []\n",
    "def predict(text,number_of_topics):\n",
    "    l = []\n",
    "   \n",
    "    blob = TextBlob(text) \n",
    "    if blob.sentiment.polarity < 0 :\n",
    "        l.append(text)\n",
    "        \n",
    "        preprocess_text(text)\n",
    "        a = vectoriseur.transform(l)\n",
    "        result = model.transform(a) #List 15 Valeur \n",
    "        liste_coeff = []\n",
    "        for i in range(14,0,-1):\n",
    "\n",
    "            print(result[0][i])\n",
    "            if result[0][i] != 0:\n",
    "                liste_coeff.append(result[0][i])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "        liste_coeff.sort(reverse=True)\n",
    "        \n",
    "\n",
    "        list_index = []\n",
    "\n",
    "        for i in range(0,len(liste_coeff)):\n",
    "            list_index.append((get_index(liste_coeff[i],result[0])))\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "   \n",
    "        for i in list_index:\n",
    "             \n",
    "            list_topics_final.append(topics[i])\n",
    "            \n",
    "            if len(list_topics_final) == number_of_topics:\n",
    "                print(\"nombre nÃ©cessaire obtenu\")\n",
    "                break\n",
    "        \n",
    "    \n",
    "    return blob.sentiment.polarity\n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "def get_index(value,liste):\n",
    "    for i in enumerate(liste):\n",
    "        if i[1] == value:\n",
    "            index = i[0]\n",
    "            \n",
    "            return index \n",
    "        \n",
    "            \n",
    "\n",
    "    \n",
    "#APP STREAMLIT\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "import streamlit as st\n",
    "\n",
    "st.title('Topic Provider')\n",
    "st.write(\"This app will provide you the topic of your sentence\")\n",
    "\n",
    "st.header('Topic(s)/Polarity')\n",
    "\n",
    "\n",
    "st.sidebar.header(\"Enter your sentence here\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "       \n",
    "text =  st.sidebar.text_input(\"Enter your sentence\",\n",
    "    \n",
    "        key=\"placeholder\")\n",
    "\n",
    "number = st.sidebar.number_input('How many keywords you want to display ? ',0,5)\n",
    "\n",
    "print(text)\n",
    "\n",
    "\n",
    "\n",
    "if text !=\"\" and number != 0:\n",
    "    a = predict(text,number)\n",
    "    \n",
    "    if len(list_topics_final) != 0:\n",
    "        \n",
    "        for i in list_topics_final:\n",
    "            st.info(i)\n",
    "    st.info(a)\n",
    "\n",
    "  \n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
